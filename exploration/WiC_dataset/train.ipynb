{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install -U sentence-transformers\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(split):\n",
    "    \"\"\"\n",
    "    Load the dataset for a given split ('train', 'dev', 'test')\n",
    "    and return a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(split)  # e.g., 'train/', 'dev/', 'test/'\n",
    "    data_file = os.path.join(base_path, f\"{split}.data.txt\")\n",
    "    label_file = os.path.join(base_path, f\"{split}.gold.txt\")\n",
    "\n",
    "    df = pd.read_csv(data_file, sep=\"\\t\", header=None, names=[\"target_word\", \"PoS\", \"indices\", \"example_1\", \"example_2\"])\n",
    "\n",
    "    df[['index1', 'index2']] = df['indices'].str.split('-', expand=True).astype(int)\n",
    "    df.drop(columns=['indices'], inplace=True)\n",
    "\n",
    "    labels = pd.read_csv(label_file, header=None, names=[\"label\"])\n",
    "    \n",
    "    df['label'] = labels['label']\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = load_dataset('train')\n",
    "dev_df = load_dataset('dev')\n",
    "test_df = load_dataset('test')\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "dev_df.to_csv('dev.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Performance:\n",
      "Accuracy: 0.5486\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.77      0.63       319\n",
      "           1       0.59      0.32      0.42       319\n",
      "\n",
      "    accuracy                           0.55       638\n",
      "   macro avg       0.56      0.55      0.52       638\n",
      "weighted avg       0.56      0.55      0.52       638\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy: 0.5364\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62       700\n",
      "           1       0.56      0.33      0.41       700\n",
      "\n",
      "    accuracy                           0.54      1400\n",
      "   macro avg       0.54      0.54      0.52      1400\n",
      "weighted avg       0.54      0.54      0.52      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple setence-concatenation Logistic Regression Model with training word2vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df[\"combined_text\"] = train_df[\"example_1\"] + \" \" + train_df[\"example_2\"]\n",
    "dev_df[\"combined_text\"] = dev_df[\"example_1\"] + \" \" + dev_df[\"example_2\"]\n",
    "test_df[\"combined_text\"] = test_df[\"example_1\"] + \" \" + test_df[\"example_2\"]\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "dev_df[\"label\"] = dev_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "test_df[\"label\"] = test_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "\n",
    "train_sentences = [sentence.split() for sentence in train_df[\"combined_text\"]]\n",
    "dev_sentences = [sentence.split() for sentence in dev_df[\"combined_text\"]]\n",
    "test_sentences = [sentence.split() for sentence in test_df[\"combined_text\"]]\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_sentence_vector(sentence, model, vector_size=100):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X_train = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in train_df[\"combined_text\"]])\n",
    "X_dev = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in dev_df[\"combined_text\"]])\n",
    "X_test = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in test_df[\"combined_text\"]])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_dev = dev_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Dev Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_dev, y_dev_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Word2Vec model...\n",
      "Transforming sentences into vectors...\n",
      "Training Logistic Regression classifier...\n",
      "\n",
      "Dev Set Performance:\n",
      "Accuracy: 0.5470\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53       319\n",
      "           1       0.54      0.58      0.56       319\n",
      "\n",
      "    accuracy                           0.55       638\n",
      "   macro avg       0.55      0.55      0.55       638\n",
      "weighted avg       0.55      0.55      0.55       638\n",
      "\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy: 0.5314\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.49      0.51       700\n",
      "           1       0.53      0.57      0.55       700\n",
      "\n",
      "    accuracy                           0.53      1400\n",
      "   macro avg       0.53      0.53      0.53      1400\n",
      "weighted avg       0.53      0.53      0.53      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Logistic Regression Model with pre-trained word2vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df[\"combined_text\"] = train_df[\"example_1\"] + \" \" + train_df[\"example_2\"]\n",
    "dev_df[\"combined_text\"] = dev_df[\"example_1\"] + \" \" + dev_df[\"example_2\"]\n",
    "test_df[\"combined_text\"] = test_df[\"example_1\"] + \" \" + test_df[\"example_2\"]\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "dev_df[\"label\"] = dev_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "test_df[\"label\"] = test_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "\n",
    "print(\"Loading pre-trained Word2Vec model...\")\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_sentence_vector(sentence, model, vector_size=300):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "print(\"Transforming sentences into vectors...\")\n",
    "X_train = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in train_df[\"combined_text\"]])\n",
    "X_dev = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in dev_df[\"combined_text\"]])\n",
    "X_test = np.array([get_sentence_vector(sentence, word2vec_model) for sentence in test_df[\"combined_text\"]])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_dev = dev_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nDev Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_dev, y_dev_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <31D2ED80-D446-353A-885A-F2032D05B554> /Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <709C1DF5-D253-3C66-87E2-C99FD3A259DF> /Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/pyarrow/lib.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '__ZN5arrow12ArrayBuilder12AppendScalarERKNS_6ScalarEx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/evaluation/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMSEEvaluatorFromDataFrame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSEEvaluatorFromDataFrame\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNanoBEIREvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NanoBEIREvaluator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParaphraseMiningEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParaphraseMiningEvaluator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRerankingEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RerankingEvaluator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/evaluation/NanoBEIREvaluator.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInformationRetrievalEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InformationRetrievalEvaluator\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/model_card.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fullname, is_accelerate_available, is_datasets_available\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict, IterableDataset, IterableDatasetDict, Value\n\u001b[1;32m     37\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.4.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/jcjustin/opt/anaconda3/lib/python3.9/site-packages/pyarrow/lib.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '__ZN5arrow12ArrayBuilder12AppendScalarERKNS_6ScalarEx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df = pd.read_csv(\"dev.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "dev_df[\"label\"] = dev_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "test_df[\"label\"] = test_df[\"label\"].map({\"T\": 1, \"F\": 0})\n",
    "\n",
    "# Step 2: Load a pre-trained sentence embedding model\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Function to compute sentence embeddings and cosine similarity\n",
    "def compute_similarity(df):\n",
    "    sent1_embeddings = model.encode(df[\"example_1\"].tolist(), convert_to_numpy=True)\n",
    "    sent2_embeddings = model.encode(df[\"example_2\"].tolist(), convert_to_numpy=True)\n",
    "    similarities = np.diag(cosine_similarity(sent1_embeddings, sent2_embeddings))\n",
    "    return similarities\n",
    "\n",
    "# Step 4: Compute cosine similarity for train, dev, and test sets\n",
    "print(\"Computing sentence similarities...\")\n",
    "X_train = compute_similarity(train_df).reshape(-1, 1)\n",
    "X_dev = compute_similarity(dev_df).reshape(-1, 1)\n",
    "X_test = compute_similarity(test_df).reshape(-1, 1)\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_dev = dev_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Step 5: Train a Logistic Regression classifier\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nDev Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_dev, y_dev_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
