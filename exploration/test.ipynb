{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"IWSLT 2017 dataset \"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m     23\u001b[0m _HOMEPAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://sites.google.com/site/iwsltevaluation2017/TED-tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m _DESCRIPTION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mThe IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT system across all directions including English, German, Dutch, Italian and Romanian. As unofficial task, conventional bilingual text translation is offered between English and Arabic, French, Japanese, Chinese, German and Korean.\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"IWSLT 2017 dataset \"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "_HOMEPAGE = \"https://sites.google.com/site/iwsltevaluation2017/TED-tasks\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "The IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT system across all directions including English, German, Dutch, Italian and Romanian. As unofficial task, conventional bilingual text translation is offered between English and Arabic, French, Japanese, Chinese, German and Korean.\n",
    "\"\"\"\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{cettolo-etal-2017-overview,\n",
    "    title = \"Overview of the {IWSLT} 2017 Evaluation Campaign\",\n",
    "    author = {Cettolo, Mauro  and\n",
    "      Federico, Marcello  and\n",
    "      Bentivogli, Luisa  and\n",
    "      Niehues, Jan  and\n",
    "      St{\\\\\"u}ker, Sebastian  and\n",
    "      Sudoh, Katsuhito  and\n",
    "      Yoshino, Koichiro  and\n",
    "      Federmann, Christian},\n",
    "    booktitle = \"Proceedings of the 14th International Conference on Spoken Language Translation\",\n",
    "    month = dec # \" 14-15\",\n",
    "    year = \"2017\",\n",
    "    address = \"Tokyo, Japan\",\n",
    "    publisher = \"International Workshop on Spoken Language Translation\",\n",
    "    url = \"https://aclanthology.org/2017.iwslt-1.1\",\n",
    "    pages = \"2--14\",\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "REPO_URL = \"https://huggingface.co/datasets/iwslt2017/resolve/main/\"\n",
    "MULTI_URL = REPO_URL + \"data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip\"\n",
    "BI_URL = REPO_URL + \"data/2017-01-trnted/texts/{source}/{target}/{source}-{target}.zip\"\n",
    "\n",
    "\n",
    "class IWSLT2017Config(datasets.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for NewDataset\"\"\"\n",
    "\n",
    "    def __init__(self, pair, is_multilingual, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            pair: the language pair to consider\n",
    "            is_multilingual: Is this pair in the multilingual dataset (download source is different)\n",
    "            **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        self.pair = pair\n",
    "        self.is_multilingual = is_multilingual\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "# XXX: Artificially removed DE from here, as it also exists within bilingual data\n",
    "MULTI_LANGUAGES = [\"en\", \"it\", \"nl\", \"ro\"]\n",
    "BI_LANGUAGES = [\"ar\", \"de\", \"en\", \"fr\", \"ja\", \"ko\", \"zh\"]\n",
    "MULTI_PAIRS = [f\"{source}-{target}\" for source in MULTI_LANGUAGES for target in MULTI_LANGUAGES if source != target]\n",
    "BI_PAIRS = [\n",
    "    f\"{source}-{target}\"\n",
    "    for source in BI_LANGUAGES\n",
    "    for target in BI_LANGUAGES\n",
    "    if source != target and (source == \"en\" or target == \"en\")\n",
    "]\n",
    "\n",
    "PAIRS = MULTI_PAIRS + BI_PAIRS\n",
    "\n",
    "\n",
    "class IWSLT217(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"The IWSLT 2017 Evaluation Campaign includes a multilingual TED Talks MT task.\"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.0.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "    BUILDER_CONFIG_CLASS = IWSLT2017Config\n",
    "    BUILDER_CONFIGS = [\n",
    "        IWSLT2017Config(\n",
    "            name=\"iwslt2017-\" + pair,\n",
    "            description=\"A small dataset\",\n",
    "            version=datasets.Version(\"1.0.0\"),\n",
    "            pair=pair,\n",
    "            is_multilingual=pair in MULTI_PAIRS,\n",
    "        )\n",
    "        for pair in PAIRS\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\"translation\": datasets.features.Translation(languages=self.config.pair.split(\"-\"))}\n",
    "            ),\n",
    "            homepage=_HOMEPAGE,\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        source, target = self.config.pair.split(\"-\")\n",
    "        if self.config.is_multilingual:\n",
    "            dl_dir = dl_manager.download_and_extract(MULTI_URL)\n",
    "            data_dir = os.path.join(dl_dir, \"DeEnItNlRo-DeEnItNlRo\")\n",
    "            years = [2010]\n",
    "        else:\n",
    "            bi_url = BI_URL.format(source=source, target=target)\n",
    "            dl_dir = dl_manager.download_and_extract(bi_url)\n",
    "            data_dir = os.path.join(dl_dir, f\"{source}-{target}\")\n",
    "            years = [2010, 2011, 2012, 2013, 2014, 2015]\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                gen_kwargs={\n",
    "                    \"source_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"train.tags.{self.config.pair}.{source}\",\n",
    "                        )\n",
    "                    ],\n",
    "                    \"target_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"train.tags.{self.config.pair}.{target}\",\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                gen_kwargs={\n",
    "                    \"source_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"IWSLT17.TED.tst{year}.{self.config.pair}.{source}.xml\",\n",
    "                        )\n",
    "                        for year in years\n",
    "                    ],\n",
    "                    \"target_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"IWSLT17.TED.tst{year}.{self.config.pair}.{target}.xml\",\n",
    "                        )\n",
    "                        for year in years\n",
    "                    ],\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                gen_kwargs={\n",
    "                    \"source_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"IWSLT17.TED.dev2010.{self.config.pair}.{source}.xml\",\n",
    "                        )\n",
    "                    ],\n",
    "                    \"target_files\": [\n",
    "                        os.path.join(\n",
    "                            data_dir,\n",
    "                            f\"IWSLT17.TED.dev2010.{self.config.pair}.{target}.xml\",\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(self, source_files, target_files):\n",
    "        \"\"\"Yields examples.\"\"\"\n",
    "        id_ = 0\n",
    "        source, target = self.config.pair.split(\"-\")\n",
    "        for source_file, target_file in zip(source_files, target_files):\n",
    "            with open(source_file, \"r\", encoding=\"utf-8\") as sf:\n",
    "                with open(target_file, \"r\", encoding=\"utf-8\") as tf:\n",
    "                    for source_row, target_row in zip(sf, tf):\n",
    "                        source_row = source_row.strip()\n",
    "                        target_row = target_row.strip()\n",
    "\n",
    "                        if source_row.startswith(\"<\"):\n",
    "                            if source_row.startswith(\"<seg\"):\n",
    "                                # Remove <seg id=\"1\">.....</seg>\n",
    "                                # Very simple code instead of regex or xml parsing\n",
    "                                part1 = source_row.split(\">\")[1]\n",
    "                                source_row = part1.split(\"<\")[0]\n",
    "                                part1 = target_row.split(\">\")[1]\n",
    "                                target_row = part1.split(\"<\")[0]\n",
    "\n",
    "                                source_row = source_row.strip()\n",
    "                                target_row = target_row.strip()\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                        yield id_, {\"translation\": {source: source_row, target: target_row}}\n",
    "                        id_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIWSLT/iwslt2017\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miwslt2017-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-en-zh\", trust_remote_code=True)\n",
    "\n",
    "num_examples = 100\n",
    "for i in range(num_examples):\n",
    "    example = dataset[\"train\"][i]\n",
    "    print(f\"English: {example['translation']['en']}\")\n",
    "    print(f\"Chinese: {example['translation']['zh']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "csv_filename = \"en-zh.csv\"\n",
    "df_train = pd.DataFrame(dataset[\"train\"][\"translation\"])\n",
    "df_train.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved dataset to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5428 5428\n",
      "      word pos index                                       sentence1  \\\n",
      "0    carry   V   2-1              You must carry your camping gear .   \n",
      "1       go   V   2-6  Messages must go through diplomatic channels .   \n",
      "2    break   V   0-2                                Break an alibi .   \n",
      "3      cup   N   8-4         He wore a jock strap with a metal cup .   \n",
      "4  academy   N   1-2                          The Academy of Music .   \n",
      "\n",
      "                                           sentence2 label  \n",
      "0                    Sound carries well over water .     F  \n",
      "1   Do you think the sofa will go through the door ?     F  \n",
      "2  The wholesaler broke the container loads into ...     F  \n",
      "3            Bees filled the waxen cups with honey .     T  \n",
      "4                               The French Academy .     F  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# open txt file in ./WiC_dataset/train/train.data.txt\n",
    "with open(\"./WiC_dataset/train/train.data.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "with open(\"./WiC_dataset/train/train.gold.txt\", \"r\") as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "data = [line.split(\"\\t\") for line in content.split(\"\\n\") if line]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"word\", \"pos\", \"index\", \"sentence1\", \"sentence2\"])\n",
    "\n",
    "print(len(df), len(labels))\n",
    "df = df.rename(columns=lambda x: x.strip()) \n",
    "df[\"label\"] = labels\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m word_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     72\u001b[0m pos_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m---> 73\u001b[0m pos_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mpos_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Combine all features\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hstack\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "\n",
    "# Load the data (you've already done this)\n",
    "# ... existing code ...\n",
    "\n",
    "# Convert labels from 'T'/'F' to 1/0\n",
    "df['label'] = df['label'].map({'T': 1, 'F': 0})\n",
    "\n",
    "# Feature engineering\n",
    "# Let's create features based on the context of the target word\n",
    "\n",
    "# Load spaCy model for NLP processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_features(row):\n",
    "    word = row['word']\n",
    "    pos = row['pos']\n",
    "    \n",
    "    # Parse the indices\n",
    "    indices = row['index'].split('-')\n",
    "    idx1 = int(indices[0])\n",
    "    idx2 = int(indices[1])\n",
    "    \n",
    "    # Tokenize sentences (simple whitespace tokenization to match dataset)\n",
    "    tokens1 = row['sentence1'].split()\n",
    "    tokens2 = row['sentence2'].split()\n",
    "    \n",
    "    # Verify the target word is at the specified position\n",
    "    target1 = tokens1[idx1].lower()\n",
    "    target2 = tokens2[idx2].lower()\n",
    "    \n",
    "    # Get context (3 words before and after, respecting sentence boundaries)\n",
    "    start1 = max(0, idx1 - 3)\n",
    "    end1 = min(len(tokens1), idx1 + 4)\n",
    "    context1 = ' '.join(tokens1[start1:idx1] + tokens1[idx1+1:end1])\n",
    "    \n",
    "    start2 = max(0, idx2 - 3)\n",
    "    end2 = min(len(tokens2), idx2 + 4)\n",
    "    context2 = ' '.join(tokens2[start2:idx2] + tokens2[idx2+1:end2])\n",
    "    \n",
    "    # Combine features\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos': pos,\n",
    "        'target1': target1,\n",
    "        'target2': target2,\n",
    "        'context1': context1,\n",
    "        'context2': context2,\n",
    "        'context_combined': context1 + ' ' + context2\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply feature extraction\n",
    "features_list = df.apply(extract_features, axis=1).tolist()\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "# Vectorize the context features\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "context_vectors = vectorizer.fit_transform(features_df['context_combined'])\n",
    "\n",
    "# Add word and POS as one-hot encoded features\n",
    "word_vectorizer = CountVectorizer()\n",
    "word_vectors = word_vectorizer.fit_transform(features_df['word'])\n",
    "\n",
    "pos_vectorizer = CountVectorizer()\n",
    "pos_vectors = pos_vectorizer.fit_transform(features_df['pos'])\n",
    "\n",
    "# Combine all features\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([context_vectors, word_vectors, pos_vectors])\n",
    "y = df['label'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of making predictions on new data\n",
    "def predict_word_sense(word, pos, sentence1, sentence2, idx1, idx2):\n",
    "    row = pd.Series({\n",
    "        'word': word,\n",
    "        'pos': pos,\n",
    "        'index': f\"{idx1}-{idx2}\",\n",
    "        'sentence1': sentence1,\n",
    "        'sentence2': sentence2\n",
    "    })\n",
    "    \n",
    "    features = extract_features(row)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    context_vec = vectorizer.transform([features['context_combined']])\n",
    "    word_vec = word_vectorizer.transform([features['word']])\n",
    "    pos_vec = pos_vectorizer.transform([features['pos']])\n",
    "    \n",
    "    X_new = hstack([context_vec, word_vec, pos_vec])\n",
    "    \n",
    "    prediction = clf.predict(X_new)[0]\n",
    "    probability = clf.predict_proba(X_new)[0][1]\n",
    "    \n",
    "    return {\n",
    "        'same_sense': bool(prediction),\n",
    "        'confidence': probability\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = predict_word_sense('bank', 'NOUN', 'I went to the bank to deposit money.', \n",
    "                           'The river bank was muddy after the rain.', 5, 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split en-zh.csv into src-train.txt and tgt-train.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'en-zh.csv'\n",
    "english_output = 'src-train.txt'\n",
    "chinese_output = 'tgt-train.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as csv_file, \\\n",
    "     open(english_output, 'w', encoding='utf-8') as eng_file, \\\n",
    "     open(chinese_output, 'w', encoding='utf-8') as zh_file:\n",
    "    \n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        if len(row) >= 2:\n",
    "            english_text = row[0].strip('\"')\n",
    "            chinese_text = row[1].strip('\"')\n",
    "            \n",
    "            # Write to respective files\n",
    "            eng_file.write(english_text + '\\n')\n",
    "            zh_file.write(chinese_text + '\\n')\n",
    "\n",
    "print(f\"Successfully split {input_file} into {english_output} and {chinese_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
