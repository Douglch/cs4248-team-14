{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape (rows, columns): (231267, 2)\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 231267\n",
      "--- Duplicates Deleted\t\t\t--> Rows: 229646\n",
      "--- Source-Copied Rows Deleted\t\t--> Rows: 229640\n",
      "--- Too Long Source/Target Deleted\t--> Rows: 3088\n",
      "--- HTML Removed\t\t\t--> Rows: 3088\n",
      "--- Rows will remain true-cased\t\t--> Rows: 3088\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 3088\n",
      "--- Rows Shuffled\t\t\t--> Rows: 3088\n",
      "--- Source Saved: ./en-zh.zh-filtered.zh\n",
      "--- Target Saved: ./en-zh.en-filtered.en\n"
     ]
    }
   ],
   "source": [
    "# filter dataset\n",
    "!python3 MT-Preparation/filtering/filter.py ./en-zh.zh ./en-zh.en zh en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.zh-filtered.zh --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.zh-filtered.zh\n",
      "  input_format: \n",
      "  model_prefix: source\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.zh-filtered.zh\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3088 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=43107\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1871\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3088 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=15491\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 7066 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3088\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 5212\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 5212 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5243 obj=26.5349 num_tokens=20865 num_tokens/piece=3.97959\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5014 obj=24.9606 num_tokens=20958 num_tokens/piece=4.1799\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: source.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: source.vocab\n",
      "Done, training a SentencepPiece model for the Source finished successfully!\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.en-filtered.en\n",
      "  input_format: \n",
      "  model_prefix: target\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.en-filtered.en\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3088 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=66990\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=75\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3088 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=32900\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 7930 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3088\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 4747\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 4747 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3536 obj=15.2875 num_tokens=12448 num_tokens/piece=3.52036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3201 obj=12.9982 num_tokens=12547 num_tokens/piece=3.91971\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: target.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: target.vocab\n",
      "Done, training a SentencepPiece model for the Target finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model for subwording\n",
    "!python3 MT-Preparation/subwording/1-train_unigram.py ./en-zh.zh-filtered.zh ./en-zh.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.zh-filtered.zh --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.zh-filtered.zh\n",
      "  input_format: \n",
      "  model_prefix: source\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.zh-filtered.zh\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3088 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=43107\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=1871\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3088 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=15491\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 7066 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3088\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 5212\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 5212 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5243 obj=26.5349 num_tokens=20865 num_tokens/piece=3.97959\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5014 obj=24.9606 num_tokens=20958 num_tokens/piece=4.1799\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: source.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: source.vocab\n",
      "Done, training a SentencepPiece model for the Source finished successfully!\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.en-filtered.en\n",
      "  input_format: \n",
      "  model_prefix: target\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.en-filtered.en\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3088 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=66990\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=75\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3088 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=32900\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 7930 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3088\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 4747\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 4747 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3536 obj=15.2875 num_tokens=12448 num_tokens/piece=3.52036\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3201 obj=12.9982 num_tokens=12547 num_tokens/piece=3.91971\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: target.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: target.vocab\n",
      "Done, training a SentencepPiece model for the Target finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# train a SentencePiece model for subword tokenization\n",
    "!python3 MT-Preparation/subwording/1-train_unigram.py ./en-zh.zh-filtered.zh ./en-zh.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Model: source.model\n",
      "Target Model: target.model\n",
      "Source Dataset: ./en-zh.zh-filtered.zh\n",
      "Target Dataset: ./en-zh.en-filtered.en\n",
      "Done subwording the source file! Output: ./en-zh.zh-filtered.zh.subword\n",
      "Done subwording the target file! Output: ./en-zh.en-filtered.en.subword\n"
     ]
    }
   ],
   "source": [
    "# subword the dataset\n",
    "!python3 MT-Preparation/subwording/2-subword.py source.model target.model ./en-zh.zh-filtered.zh ./en-zh.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "它正在发生着。\n",
      "43次。\n",
      "因为我们已经看到了PS 234 的项目是怎么做的。\n",
      "-----\n",
      "It's happening.\n",
      "Forty-three.\n",
      "We saw how PS 234 worked.\n"
     ]
    }
   ],
   "source": [
    "# first 3 lines before subwording\n",
    "!head -n 3 ./en-zh.zh-filtered.zh && echo \"-----\" && head -n 3 ./en-zh.en-filtered.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁它 正在 发生 着 。\n",
      "▁ 4 3 次 。\n",
      "▁因为 我们已经 看到了 P S ▁ 2 3 4 ▁ 的项目 是怎么做 的 。\n",
      "---\n",
      "head: ./en-zh.en-filtered.en.subwordafter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# first 3 lines after subwording\n",
    "!head -n 3 ./en-zh.zh-filtered.zh.subword && echo \"---\" && head -n 3 ./en-zh.en-filtered.en.subwordafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (3088, 2)\n",
      "--- Empty Cells Deleted --> Rows: 3088\n",
      "--- Wrote Files\n",
      "Done!\n",
      "Output files\n",
      "./en-zh.zh-filtered.zh.subword.train\n",
      "./en-zh.en-filtered.en.subword.train\n",
      "./en-zh.zh-filtered.zh.subword.dev\n",
      "./en-zh.en-filtered.en.subword.dev\n",
      "./en-zh.zh-filtered.zh.subword.test\n",
      "./en-zh.en-filtered.en.subword.test\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training set, development set, and test set\n",
    "# Development and test sets should be between 100 and 500 segments (here we chose 200)\n",
    "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 200 200 ./en-zh.zh-filtered.zh.subword ./en-zh.en-filtered.en.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     200 ./en-zh.en-filtered.en.subword.dev\n",
      "     200 ./en-zh.en-filtered.en.subword.test\n",
      "    2688 ./en-zh.en-filtered.en.subword.train\n",
      "     200 ./en-zh.zh-filtered.zh.subword.dev\n",
      "     200 ./en-zh.zh-filtered.zh.subword.test\n",
      "    2688 ./en-zh.zh-filtered.zh.subword.train\n",
      "    6176 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./*.subword.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---First line---\n",
      "==> ./en-zh.en-filtered.en.subword.train <==\n",
      "▁It ' s ▁happening .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.train <==\n",
      "▁它 正在 发生 着 。\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.dev <==\n",
      "▁We ▁call ▁it ▁O ME GA , ▁wh ich ▁is ▁an ▁ac r onym ▁for ▁Off shor e ▁Mem bran e ▁Enc los ures ▁for ▁Gr owing ▁Alg a e .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.dev <==\n",
      "▁我们 称 之 为 O ME GA ▁是 O ff s hore ▁Me mbr ane ▁Enc lo sure s ▁f or ▁G row ing ▁Al ga e ▁ 的 缩 写\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.test <==\n",
      "▁S ign ▁language .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.test <==\n",
      "▁ 手 语 。\n",
      "\n",
      "---Last line---\n",
      "==> ./en-zh.en-filtered.en.subword.train <==\n",
      "▁Miss !\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.train <==\n",
      "▁“ 老师 !”\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.dev <==\n",
      "▁All ▁right , ▁next .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.dev <==\n",
      "▁好 , ▁下一个 。\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.test <==\n",
      "▁Yes , ▁M ama .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.test <==\n",
      "▁好的 , 妈妈\n"
     ]
    }
   ],
   "source": [
    "# check the first and last line from each dataset\n",
    "!echo \"---First line---\"\n",
    "!head -n 1 ./*.{train,dev,test}\n",
    "\n",
    "!echo -e \"\\n---Last line---\"\n",
    "!tail -n 1 ./*.{train,dev,test}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
