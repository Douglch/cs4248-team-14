{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape (rows, columns): (231267, 2)\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 231267\n",
      "--- Duplicates Deleted\t\t\t--> Rows: 229646\n",
      "--- Source-Copied Rows Deleted\t\t--> Rows: 229640\n",
      "--- Too Long Source/Target Deleted\t--> Rows: 229604\n",
      "--- HTML Removed\t\t\t--> Rows: 229604\n",
      "--- Rows will remain true-cased\t\t--> Rows: 229604\n",
      "--- Rows with Empty Cells Deleted\t--> Rows: 229604\n",
      "--- Rows Shuffled\t\t\t--> Rows: 229604\n",
      "--- Source Saved: ./en-zh.zh-filtered.zh\n",
      "--- Target Saved: ./en-zh.en-filtered.en\n"
     ]
    }
   ],
   "source": [
    "# filter dataset\n",
    "!python3 MT-Preparation/filtering/filter.py ./en-zh.en ./en-zh.zh en zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.zh-filtered.zh --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.zh-filtered.zh\n",
      "  input_format: \n",
      "  model_prefix: source\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.zh-filtered.zh\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 229604 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=7407672\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=3637\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 229604 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=2144608\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1003637 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 229604\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 546127\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 546127 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=485928 obj=51.223 num_tokens=2640703 num_tokens/piece=5.43435\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=434577 obj=47.3836 num_tokens=2653390 num_tokens/piece=6.10568\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=324876 obj=47.7668 num_tokens=2752591 num_tokens/piece=8.47274\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=323013 obj=47.491 num_tokens=2755175 num_tokens/piece=8.52961\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=242033 obj=48.2593 num_tokens=2865217 num_tokens/piece=11.8381\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=241835 obj=47.9848 num_tokens=2866721 num_tokens/piece=11.854\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=181357 obj=48.8973 num_tokens=2983972 num_tokens/piece=16.4536\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=181311 obj=48.6079 num_tokens=2985623 num_tokens/piece=16.4669\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=135982 obj=49.5558 num_tokens=3090492 num_tokens/piece=22.7272\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=135976 obj=49.3103 num_tokens=3091658 num_tokens/piece=22.7368\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=101981 obj=50.2919 num_tokens=3196278 num_tokens/piece=31.3419\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=101980 obj=50.0663 num_tokens=3197489 num_tokens/piece=31.3541\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=76484 obj=51.0698 num_tokens=3302661 num_tokens/piece=43.1811\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=76483 obj=50.8543 num_tokens=3303431 num_tokens/piece=43.1917\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=57361 obj=51.9228 num_tokens=3413702 num_tokens/piece=59.5126\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=57361 obj=51.6946 num_tokens=3415032 num_tokens/piece=59.5358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=55000 obj=51.8534 num_tokens=3432437 num_tokens/piece=62.4079\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55000 obj=51.8185 num_tokens=3432665 num_tokens/piece=62.4121\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: source.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: source.vocab\n",
      "Done, training a SentencepPiece model for the Source finished successfully!\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./en-zh.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./en-zh.en-filtered.en\n",
      "  input_format: \n",
      "  model_prefix: target\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./en-zh.en-filtered.en\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 229604 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=22134805\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9599% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999599\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 229604 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=11513883\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 177767 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 229604\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 132403\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 132403 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61193 obj=10.6674 num_tokens=287143 num_tokens/piece=4.69242\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51145 obj=8.24283 num_tokens=287639 num_tokens/piece=5.62399\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: target.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: target.vocab\n",
      "Done, training a SentencepPiece model for the Target finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model for subwording\n",
    "!python3 MT-Preparation/subwording/1-train_unigram.py ./en-zh.en-filtered.en ./en-zh.zh-filtered.zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Model: source.model\n",
      "Target Model: target.model\n",
      "Source Dataset: ./en-zh.zh-filtered.zh\n",
      "Target Dataset: ./en-zh.en-filtered.en\n",
      "Done subwording the source file! Output: ./en-zh.zh-filtered.zh.subword\n",
      "Done subwording the target file! Output: ./en-zh.en-filtered.en.subword\n"
     ]
    }
   ],
   "source": [
    "# subword the dataset\n",
    "!python3 MT-Preparation/subwording/2-subword.py source.model target.model ./en-zh.en-filtered.en ./en-zh.zh-filtered.zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们不使用化石能源\n",
      "正如你所知的， 我们让免疫系统对这个奇怪的分子产生了反应， 而这个分子并不是我们做的，我们能在其他动物和物体上看到这样的物质。\n",
      "这么多年来，这个国家 因政治和宗教原因支离破碎\n",
      "-----\n",
      "We are not using fossil energies.\n",
      "Now, I was thinking about that and I said, you know, we've got this immune response to this ridiculous molecule that we don't make, and we see it a lot in other animals and stuff.\n",
      "For years, the country has been divided between politics and religion.\n"
     ]
    }
   ],
   "source": [
    "# first 3 lines before subwording\n",
    "!head -n 3 ./en-zh.en-filtered.en && echo \"-----\" && head -n 3 ./en-zh.zh-filtered.zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁我们 不使用 化石 能源\n",
      "▁正如你 所知的 , ▁我们让 免疫系统 对这个 奇怪的 分子 产生了 反应 , ▁而这个 分子 并不是 我们做的 , 我们能 在 其他动物 和 物体上 看到这样的 物质 。\n",
      "▁ 这么多年来 , 这个国家 ▁因 政治和 宗教 原因 支离破碎\n",
      "---\n",
      "==> ./en-zh.en-filtered.en.subword <==\n",
      "▁We ▁are ▁not ▁using ▁fossil ▁energies .\n",
      "▁Now , ▁I ▁was ▁thinking ▁about ▁that ▁and ▁I ▁said , ▁you ▁know , ▁we ' ve ▁got ▁this ▁immune ▁response ▁to ▁this ▁ridiculous ▁molecule ▁that ▁we ▁don ' t ▁make , ▁and ▁we ▁see ▁it ▁a ▁lot ▁in ▁other ▁animals ▁and ▁stuff .\n",
      "▁For ▁years , ▁the ▁country ▁has ▁been ▁divided ▁between ▁politics ▁and ▁religion .\n",
      "head: after: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# first 3 lines after subwording\n",
    "!head -n 3 ./en-zh.en-filtered.en.subword && echo \"---\" && head -n 3 ./en-zh.zh-filtered.zh.subword after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (229604, 2)\n",
      "--- Empty Cells Deleted --> Rows: 229604\n",
      "--- Wrote Files\n",
      "Done!\n",
      "Output files\n",
      "./en-zh.zh-filtered.zh.subword.train\n",
      "./en-zh.en-filtered.en.subword.train\n",
      "./en-zh.zh-filtered.zh.subword.dev\n",
      "./en-zh.en-filtered.en.subword.dev\n",
      "./en-zh.zh-filtered.zh.subword.test\n",
      "./en-zh.en-filtered.en.subword.test\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training set, development set, and test set\n",
    "# Development and test sets should be between 1000 and 5000 segments (here we chose 200)\n",
    "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 ./en-zh.en-filtered.en.subword ./en-zh.zh-filtered.zh.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2000 ./en-zh.en-filtered.en.subword.dev\n",
      "    2000 ./en-zh.en-filtered.en.subword.test\n",
      "     200 ./en-zh.en-filtered.en.subword.test.desubword\n",
      "  225604 ./en-zh.en-filtered.en.subword.train\n",
      "     200 ./en-zh.en-filtered.en.subword.translated\n",
      "     200 ./en-zh.en-filtered.en.subword.translated.desubword\n",
      "    2000 ./en-zh.zh-filtered.zh.subword.dev\n",
      "    2000 ./en-zh.zh-filtered.zh.subword.test\n",
      "  225604 ./en-zh.zh-filtered.zh.subword.train\n",
      "  459808 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./*.subword.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---First line---\n",
      "==> ./en-zh.en-filtered.en.subword.train <==\n",
      "▁We ▁are ▁not ▁using ▁fossil ▁energies .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.train <==\n",
      "▁我们 不使用 化石 能源\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.dev <==\n",
      "▁I ▁can ' t ▁take ▁credit ▁for ▁Christopher ' s ▁journey ▁to ▁success , ▁but ▁I ▁certainly ▁did ▁my ▁part ▁to ▁keep ▁him ▁on ▁the ▁path .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.dev <==\n",
      "▁我不能 说 克里斯托弗 的成功 完全是 因为我 , ▁但我 的确 将他 带到了 正确 的道路上 。\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.test <==\n",
      "▁Everybody ▁wins .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.test <==\n",
      "▁ 共 赢\n",
      "\n",
      "---Last line---\n",
      "==> ./en-zh.en-filtered.en.subword.train <==\n",
      "▁This ▁was ▁a ▁headline ▁in ▁a ▁U . K . ▁newspaper , ▁The ▁Guardian , ▁not ▁that ▁long ▁ago .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.train <==\n",
      "▁这句话 是一家 英国 报纸 的头条 , ▁不久 之前 出版的 《 卫报 》。\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.dev <==\n",
      "▁But ▁anyway , ▁the ▁council ▁granted ▁these ▁people ▁permission , ▁and ▁they ' re ▁now ▁trying ▁to ▁build ▁their ▁cemetery .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.dev <==\n",
      "▁但 不管怎么说 , ▁这些人 获得了 地方 议会 的 批准 ▁他们正在 试着 建造 他们的 墓园\n",
      "\n",
      "==> ./en-zh.en-filtered.en.subword.test <==\n",
      "▁Which ▁is ▁a ▁feel - good , ▁calming , ▁stress - reducing ▁hormone .\n",
      "\n",
      "==> ./en-zh.zh-filtered.zh.subword.test <==\n",
      "▁是 能使 人 感觉良好 、 冷静 、 ▁ 降低 压力 的 荷尔蒙 。\n"
     ]
    }
   ],
   "source": [
    "# check the first and last line from each dataset\n",
    "!echo \"---First line---\"\n",
    "!head -n 1 ./*.{train,dev,test}\n",
    "\n",
    "!echo -e \"\\n---Last line---\"\n",
    "!tail -n 1 ./*.{train,dev,test}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
